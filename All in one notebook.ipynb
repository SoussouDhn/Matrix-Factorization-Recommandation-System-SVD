{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Mat-Fact.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yjt1bsLWbKh-"
      },
      "source": [
        "# Project steps explained\r\n",
        "In this notebook we will explain the different steps f our approach"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Ki-cjqQbGi3"
      },
      "source": [
        "import numpy as np\r\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "47br_RImrh0G"
      },
      "source": [
        "# loading first table\r\n",
        "bookmarks=pd.read_csv(\"/content/drive/My Drive/Colab Notebooks/bookmarks.csv\")\r\n",
        "bookmarks.time = 1\t# we set this column to one, because we are gonna use it later in calculating interest\r\n",
        "\r\n",
        "# loading second table\r\n",
        "favorites=pd.read_csv(\"/content/drive/My Drive/Colab Notebooks/favorites.csv\")\r\n",
        "favorites.added_date = 5 # we set this column to 5, because we are gonna use it later in calculating interest\r\n",
        "\r\n",
        "#loading the third table\r\n",
        "ratings=pd.read_csv(\"/content/drive/My Drive/Colab Notebooks/ratings.csv\")\r\n",
        "ratings=ratings.drop(columns=['time'])\t# we drop this column, cause it has no use to us"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TedNiVRAyat7"
      },
      "source": [
        "# with this, we merge the two tables, adding the scores +5 of favorite to the adgecent lines\r\n",
        "bookmarks = bookmarks.merge(favorites,how='left', \r\n",
        "\t\t\tleft_on=['id_profile','id_asset'], right_on=['id_profile','id_asset'])\r\n",
        "\r\n",
        "# with this, we merge the two tables, adding the ratings of movies to the adgecent lines\r\n",
        "bookmarks = bookmarks.merge(ratings,how='left', \r\n",
        "\t\t\tleft_on=['id_profile','id_asset'], right_on=['id_profile','id_asset'])\r\n",
        "\r\n",
        "# filling al he na values with 0, to facilitate the calculation\r\n",
        "bookmarks=bookmarks.fillna(0)\r\n",
        "# renaming so it would make beter sense\r\n",
        "bookmarks=bookmarks.rename(columns={\"time\": \"m_ui\", \"added_date\": \"f_ui\",\"score\":\"n_ui\"})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WDCtlOCSXdzk"
      },
      "source": [
        "# calculating interest\r\n",
        "bookmarks.m_ui += bookmarks.f_ui + bookmarks.n_ui "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hPWYhboCX7Ea"
      },
      "source": [
        "bookmarks = bookmarks.drop(columns=['f_ui',\t'n_ui']).rename(columns={\"m_ui\": \"r_ui\"})\r\n",
        "bookmarks"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "amrUbRs4c8kL"
      },
      "source": [
        "bookmarks.to_csv(path_or_buf=\"/content/drive/My Drive/Colab Notebooks/D.csv\"\r\n",
        ",index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ozfY58hvdiWz"
      },
      "source": [
        "D = pd.read_csv(\"/content/drive/My Drive/Colab Notebooks/D.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CxMfj-_Mt6EY"
      },
      "source": [
        "# loading indexes\r\n",
        "train_idx = np.load(\"/content/drive/My Drive/Colab Notebooks/bookmarks_idx_train.npy\")\r\n",
        "test_idx = np.load(\"/content/drive/My Drive/Colab Notebooks/bookmarks_idx_test.npy\")\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AqpdaZ3_VzyH"
      },
      "source": [
        "D_train = D.loc[train_idx]\r\n",
        "D_train.reset_index(inplace=True)\r\n",
        "D_train.to_csv(path_or_buf=\"/content/drive/My Drive/Colab Notebooks/D_train.csv\"\r\n",
        ",index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PFzF8gkqwCbp"
      },
      "source": [
        "D_test = D.loc[test_idx]\r\n",
        "D_test.reset_index(inplace=True)\r\n",
        "D_test.to_csv(path_or_buf=\"/content/drive/My Drive/Colab Notebooks/D_test.csv\"\r\n",
        ",index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tUUvr-_cwXhj"
      },
      "source": [
        "# Reading the trainig data set \r\n",
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "D_train = pd.read_csv(\"/content/drive/My Drive/Colab Notebooks/D_train.csv\")\r\n",
        "D_train= D_train.drop(columns=['index'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "spbID7LV8gEX"
      },
      "source": [
        "# Baseline Estimates"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EjvlGoQm8h8L"
      },
      "source": [
        "user_set = set(D_train.id_profile)\r\n",
        "movies_set = set(D_train.id_asset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tNZeRnIjF0iQ"
      },
      "source": [
        "b_u_pd = pd.DataFrame(user_set, columns=['id_profile'])\r\n",
        "b_u_pd['bu'] = 0.0 \r\n",
        "print(b_u_pd)\r\n",
        "\r\n",
        "\r\n",
        "b_i_pd = pd.DataFrame(movies_set,\r\n",
        "                   columns=['id_asset'])\r\n",
        "b_i_pd['bi'] = 0.0\r\n",
        "print(b_i_pd)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sPBRLcc9F-KU"
      },
      "source": [
        "mu = np.mean(D_train.r_ui)\r\n",
        "lr = 0.0001\r\n",
        "reg_coeif = 0.02\r\n",
        "epochs = 5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SLYVpr2h4FR9"
      },
      "source": [
        "D_train['err'] = 0.0\r\n",
        "D_train = D_train.merge(b_u_pd,how='left', left_on=['id_profile'], right_on=['id_profile'])\r\n",
        "D_train = D_train.merge(b_i_pd,how='left', left_on=['id_asset'], right_on=['id_asset'])\r\n",
        "D_train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D7FvFkpPeeqj"
      },
      "source": [
        "mse_track = list()\r\n",
        "for i in range(0, 15):\r\n",
        "  \r\n",
        "  D_train['err'] = D_train['r_ui'] - D_train['bu'] - D_train['bi'] - mu\r\n",
        "\r\n",
        "  D_train['bu'] = (-2 * D_train['err'] + 2 * reg_coeif * D_train['bu'])\r\n",
        "  D_train['bi'] =  (-2 *D_train['err'] + 2 * reg_coeif * D_train['bi'])\r\n",
        "\r\n",
        "  b_u_pd['bu'] -= 1/len(D_train) * lr * D_train.groupby([\"id_profile\"]).bu.sum().reset_index()['bu']\r\n",
        "  b_i_pd['bi'] -= 1/len(D_train) * lr * D_train.groupby([\"id_asset\"]).bi.sum().reset_index()['bi']\r\n",
        "\r\n",
        "  D_train= D_train.drop(columns=['bu','bi'])\r\n",
        "\r\n",
        "  D_train = D_train.merge(b_u_pd,how='left', left_on=['id_profile'], right_on=['id_profile'])\r\n",
        "  D_train = D_train.merge(b_i_pd,how='left', left_on=['id_asset'], right_on=['id_asset'])\r\n",
        "\r\n",
        "  mse_track.append(1/len(D_train) * ((D_train['err']**2).sum()))\r\n",
        "  print(mse_track)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GfM-nLZPj8JL"
      },
      "source": [
        "b_u_pd.to_csv(path_or_buf=\"/content/drive/My Drive/Colab Notebooks/baseline_bu.csv\"\r\n",
        ",index=False)\r\n",
        "b_i_pd.to_csv(path_or_buf=\"/content/drive/My Drive/Colab Notebooks/baseline_bi.csv\"\r\n",
        ",index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vfK9q40okljd"
      },
      "source": [
        "# loading test set\r\n",
        "D_test = pd.read_csv(\"/content/drive/My Drive/Colab Notebooks/D_test.csv\")\r\n",
        "b_u_pd = pd.read_csv(\"/content/drive/My Drive/Colab Notebooks/baseline_bu.csv\")\t#loading weights\r\n",
        "b_i_pd = pd.read_csv(\"/content/drive/My Drive/Colab Notebooks/baseline_bi.csv\")\t#loading weights\r\n",
        "\r\n",
        "# calculating mu, setting up error column, associating weights to lines\r\n",
        "mu = np.mean(D_test.r_ui)\r\n",
        "#initiali\r\n",
        "D_test['err'] = 0.0\r\n",
        "D_test = D_test.merge(b_u_pd,how='left', left_on=['id_profile'], right_on=['id_profile'])\r\n",
        "D_test = D_test.merge(b_i_pd,how='left', left_on=['id_asset'], right_on=['id_asset'])\r\n",
        "# calculating the error for all the 22M entries\r\n",
        "D_test['err'] = D_test['r_ui'] - D_test['bu'] - D_test['bi'] - mu\r\n",
        "# MSE: 0.55\r\n",
        "print(1/len(D_test) * ((D_test['err']**2).sum()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33b0IjvAqVHZ"
      },
      "source": [
        "# SVD"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uq7boeYnPUyG"
      },
      "source": [
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "D_train = pd.read_csv(\"/content/drive/My Drive/Colab Notebooks/D_train.csv\")\r\n",
        "\r\n",
        "#ndiroha oumba3dddaaa \r\n",
        "D_train= D_train.drop(columns=['index'])\r\n",
        "print(D_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hs8E6wptX5yf"
      },
      "source": [
        "# prepocesing of D_train\r\n",
        "mu = np.mean(D_train.r_ui)\r\n",
        "lr = 0.00001\r\n",
        "reg_coeif = 0.02\r\n",
        "epochs = 5\r\n",
        "factor = 5\r\n",
        "\r\n",
        "user_set = set(D_train.id_profile)\r\n",
        "movies_set = set(D_train.id_asset)\r\n",
        "\r\n",
        "b_u_pd = pd.DataFrame(user_set, columns=['id_profile'])\r\n",
        "b_u_pd['bu'] = 0.0 \r\n",
        "b_i_pd = pd.DataFrame(movies_set,columns=['id_asset'])\r\n",
        "b_i_pd['bi'] = 0.0\r\n",
        "\r\n",
        "#initialisation de PU\r\n",
        "p_u_pd = pd.DataFrame(user_set,columns=['id_profile'])\r\n",
        "p_u_pd['pu'] = p_u_pd.apply(lambda x: np.random.rand(factor).tolist(), axis=1)\r\n",
        "\r\n",
        "#initialisation de Qi\r\n",
        "q_i_pd = pd.DataFrame(movies_set,columns=['id_asset'])\r\n",
        "q_i_pd['qi'] = q_i_pd.apply(lambda x: np.random.rand(factor).tolist(), axis=1)\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t3OMPqPz6tcn"
      },
      "source": [
        "def dot_pu_qi(x, y):\r\n",
        "    return np.dot(np.asarray(x), np.asarray(y))\r\n",
        "\r\n",
        "def calculate_delta(x, y, z):\r\n",
        "    return ( lr * (-2 * z * np.asarray(x) + 2 * reg_coeif * np.asarray(y))).tolist()\r\n",
        "\r\n",
        "from functools import reduce\r\n",
        "def test_sum(series):\r\n",
        "  return reduce(lambda x, y: (np.asarray(x) + np.asarray(y)).tolist(), series)\r\n",
        "\r\n",
        "def sub_vec(x, y):\r\n",
        "    return (np.asarray(x) - np.asarray(y)).tolist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rwp9sxPhiTnU"
      },
      "source": [
        "batch_size = 2000000\r\n",
        "n_batchs = len(D_train)/batch_size\r\n",
        "mse_track = list()\r\n",
        "#for i in range(epochs):\r\n",
        "for j in range(1):\r\n",
        "  Batch = D_train.loc[j*batch_size:(j+1)*batch_size-1]\r\n",
        "\r\n",
        "  # adding columns needed for calculating the error\r\n",
        "  Batch['err'] = 0.0\r\n",
        "  Batch = Batch.merge(b_u_pd,how='left', left_on=['id_profile'], right_on=['id_profile'])\r\n",
        "  Batch = Batch.merge(b_i_pd,how='left', left_on=['id_asset'], right_on=['id_asset'])\r\n",
        "  Batch = Batch.merge(p_u_pd,how='left', left_on=['id_profile'], right_on=['id_profile'])\r\n",
        "  Batch = Batch.merge(q_i_pd,how='left', left_on=['id_asset'], right_on=['id_asset'])\r\n",
        "\r\n",
        "  # calculating the error\r\n",
        "  Batch['err'] = Batch['r_ui'] - Batch['bu'] - Batch['bi'] - mu - Batch[[\"pu\", \"qi\"]].apply(lambda x : dot_pu_qi(*x), axis=1)\r\n",
        "\r\n",
        "  # calculating the error in regard to the parameters\r\n",
        "  Batch['bu'] = 1/200000 * lr * (-2 * Batch['err'] + 2 * reg_coeif * Batch['bu'])\r\n",
        "  Batch['bi'] = 1/200000 * lr * (-2 *Batch['err'] + 2 * reg_coeif * Batch['bi'])\r\n",
        "  Batch[\"delta_pu\"] = Batch[[\"qi\", \"pu\", \"err\"]].apply(lambda x : calculate_delta(*x), axis=1)\r\n",
        "  Batch[\"delta_qi\"] = Batch[[\"pu\", \"qi\", \"err\"]].apply(lambda x : calculate_delta(*x), axis=1)\r\n",
        "\r\n",
        "  # updating the b_u weights, todoso we create a dataframe containing the error than substract it \r\n",
        "  delta_bu = (Batch.groupby([\"id_profile\"]).bu.sum().reset_index()).rename(columns={\"bu\": \"bu_delta\"})\r\n",
        "  b_u_pd = (b_u_pd.merge(delta_bu,how='left', left_on=['id_profile'], right_on=['id_profile'])).fillna(0)\r\n",
        "  b_u_pd['bu'] -= b_u_pd['bu_delta']\r\n",
        "  b_u_pd= b_u_pd.drop(columns=['bu_delta'])\r\n",
        "\r\n",
        "  # updating the b_i weights, todoso we create a dataframe containing the error than substract it \r\n",
        "  delta_bi = (Batch.groupby([\"id_asset\"]).bi.sum().reset_index()).rename(columns={\"bi\": \"bi_delta\"})\r\n",
        "  b_i_pd = (b_i_pd.merge(delta_bi,how='left', left_on=['id_asset'], right_on=['id_asset'])).fillna(0)\r\n",
        "  b_i_pd['bi'] -= b_i_pd['bi_delta']\r\n",
        "  b_i_pd= b_i_pd.drop(columns=['bi_delta'])\r\n",
        "\r\n",
        "  # updating the p_u weights, todoso we create a dataframe containing the error than substract it \r\n",
        "  delta_pu = Batch[['id_profile', 'delta_pu']].groupby('id_profile').agg({'delta_pu': [test_sum]})\r\n",
        "  delta_pu.columns = delta_pu.columns.droplevel(1)\r\n",
        "  p_u_pd = (p_u_pd.merge(delta_pu,how='left', left_on=['id_profile'], right_on=['id_profile'])).fillna(0)\r\n",
        "  p_u_pd[\"pu\"] = p_u_pd[[\"pu\", \"delta_pu\"]].apply(lambda x : sub_vec(*x), axis=1)\r\n",
        "  p_u_pd= p_u_pd.drop(columns=['delta_pu'])\r\n",
        "\r\n",
        "  # updating the q_i weights, todoso we create a dataframe containing the error than substract it \r\n",
        "  delta_qi = Batch[['id_asset', 'delta_qi']].groupby('id_asset').agg({'delta_qi': [test_sum]})\r\n",
        "  delta_qi.columns = delta_qi.columns.droplevel(1)\r\n",
        "  q_i_pd = (q_i_pd.merge(delta_qi,how='left', left_on=['id_asset'], right_on=['id_asset'])).fillna(0)\r\n",
        "  q_i_pd[\"qi\"] = q_i_pd[[\"qi\", \"delta_qi\"]].apply(lambda x : sub_vec(*x), axis=1)\r\n",
        "  q_i_pd= q_i_pd.drop(columns=['delta_qi'])\r\n",
        "\r\n",
        "  \r\n",
        "  #vérfier pour les 400 derniers individus \r\n",
        "  ############################\r\n",
        "  '''b_u_pd['bu'] -= 1/len(D_train) * D_train.groupby([\"id_profile\"]).bu.sum().reset_index()['bu']\r\n",
        "  b_i_pd['bi'] -= 1/len(D_train) * D_train.groupby([\"id_asset\"]).bi.sum().reset_index()['bi']'''\r\n",
        "  #D_train= D_train.drop(columns=['bu','bi', 'pu', 'qi', ])\r\n",
        "\r\n",
        "\r\n",
        "  \r\n",
        "  mse_track.append(1/len(Batch) * ((Batch['err']**2).sum()))\r\n",
        "  print(mse_track)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "33Z9uvT1ruP9"
      },
      "source": [
        "b_u_pd.to_csv(path_or_buf=\"/content/drive/My Drive/Colab Notebooks/SVD_bu.csv\" ,index=False)\r\n",
        "b_i_pd.to_csv(path_or_buf=\"/content/drive/My Drive/Colab Notebooks/SVD_bi.csv\" ,index=False)\r\n",
        "p_u_pd.to_csv(path_or_buf=\"/content/drive/My Drive/Colab Notebooks/SVD_pu.csv\" ,index=False)\r\n",
        "q_i_pd.to_csv(path_or_buf=\"/content/drive/My Drive/Colab Notebooks/SVD_qi.csv\" ,index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-PbVqkKQsp0N"
      },
      "source": [
        "# loading test set\r\n",
        "D_test = pd.read_csv(\"/content/drive/My Drive/Colab Notebooks/D_test.csv\")\r\n",
        "b_u_pd = pd.read_csv(\"/content/drive/My Drive/Colab Notebooks/SVD_bu.csv\")\t#loading weights\r\n",
        "b_i_pd = pd.read_csv(\"/content/drive/My Drive/Colab Notebooks/SVD_bi.csv\")\t#loading weights\r\n",
        "p_u_pd = pd.read_csv(\"/content/drive/My Drive/Colab Notebooks/SVD_bu.csv\")\t#loading weights\r\n",
        "q_i_pd = pd.read_csv(\"/content/drive/My Drive/Colab Notebooks/SVD_bi.csv\")\t#loading weights\r\n",
        "\r\n",
        "# calculating mu, setting up error column, associating weights to lines\r\n",
        "mu = np.mean(D_test.r_ui)\r\n",
        "#initiali\r\n",
        "D_test = D_test.merge(b_u_pd,how='left', left_on=['id_profile'], right_on=['id_profile'])\r\n",
        "D_test = D_test.merge(b_i_pd,how='left', left_on=['id_asset'], right_on=['id_asset'])\r\n",
        "D_test = D_test.merge(p_u_pd,how='left', left_on=['id_profile'], right_on=['id_profile'])\r\n",
        "D_test = D_test.merge(q_i_pd,how='left', left_on=['id_asset'], right_on=['id_asset'])\r\n",
        "\r\n",
        "def dot_pu_qi(x, y):\r\n",
        "    return np.dot(np.asarray(x), np.asarray(y))\r\n",
        "\r\n",
        "# calculating the error\r\n",
        "D_test['err'] = D_test['r_ui'] - D_test['bu'] - D_test['bi'] - mu - D_test[[\"pu\", \"qi\"]].apply(lambda x : dot_pu_qi(*x), axis=1)\r\n",
        "# MSE: 2.11\r\n",
        "print(1/len(D_test) * ((D_test['err']**2).sum()))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}